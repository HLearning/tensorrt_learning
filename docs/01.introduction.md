# 1. 简介

NVIDIA TensorRT 是一个高性能机器学习推理的SDK。它旨在与TensorFlow、PyTorch 和 MXNet 等训练框架以互补的方式工作。它的重点在于如何在NVIDIA硬件上快速高效地运行已经训练过的网络。

有关如何安装TensorRT的说明，请参阅《NVIDIA TensorRT安装指南》。

NVIDIA TensorRT 快速入门指南适用于想要试用 TensorRT SDK 的用户；具体来说，您将学习如何构造一个快速在TensorRT 引擎上运行推理的程序。

## 1.1. 指南的结构
- 第1章提供了有关如何打包和支持TensorRT的信息，以及它如何融入开发人员生态系统。
- 第2章概述了TensorRT的功能。
- 第三章和第四章分别介绍了C++和Python API。
- 后续章节提供了有关高级功能的更多详细信息。
- 附录包含图层参考和常见问题解答。

## 1.2. 样例
《NVIDIA TensorRT 样例支持指南》阐述了本指南中讨论的许多主题。这里可以找到更多关于嵌入式应用程序的示例。

## 1.3. 补充GPU功能
多实例GPU或MIG是具有NVIDIA AMPLE架构或更高架构的NVIDIA GPU的一项功能，该架构支持用户将单个GPU直接划分为多个较小的GPU。物理分区提供具有QoS的专用计算和内存切片，并在GPU的部分上独立执行并行工作负载。对于GPU利用率较低的TensorRT应用程序，MIG可以在对延迟影响较小或没有影响的情况下产生更高的吞吐量。最佳分区方案是特定于应用程序的。

## 1.4. 补充软件

英伟达Triton 推理服务器是一个高级库，提供跨CPU和GPU的优化推理。它提供了启动和管理多个模型的功能，以及用于服务推理的REST和gRPC端点。

NVIDIA DALI®提供用于预处理图像、音频和视频数据的高性能原语。TensorRT推理可以集成为DALI管道中的自定义运算符。这里可以找到作为DALI一部分集成的TensorRT推理的工作示例。

TensorFlow TensorRT（TF-TRT）是将TensorRT直接集成到TensorFlow中。它选择要由TensorRT加速的TensorFlow图的子图，而剩下的图则由TensorFlow本地执行。结果仍然是一个可以像往常一样执行的张量流图。有关TF-TRT示例，请参阅TensorFlow中TensorRT的示例。

PyTorch量化工具包提供了以较低精度训练模型的工具，然后可以将其导出以在TensorRT中进行优化。

此外，PyTorch自动稀疏（ASP）工具为具有结构化稀疏性的训练模型提供了便利，然后可以将其导出，并允许TensorRT在NVIDIA AMPLE架构GPU上使用更快的稀疏策略。

TensorRT与NVIDIA的分析工具NVIDIA Nsight集成™ Systems and NVIDIA®Deep Learning Profiler（DLProf）。

TensorRT的受限子集经认证可用于NVIDIA DRIVE®产品。一些API标记为仅在NVIDIA驱动器中使用，不支持通用。

## 1.5. ONNX公司
TensorRT从框架导入经过训练的模型的主要方法是通过ONNX交换格式。TensorRT附带了一个ONNX解析器库来帮助导入模型。在可能的情况下，解析器向后兼容到opset 7；ONNX模型Opset版本转换器可以帮助解决不兼容问题。


GitHub版本可能支持比TensorRT附带的版本更高的opset。有关支持的opset和运算符的最新信息，请参阅ONNX TensorRT运算符支持矩阵。


TensorRT的ONNX操作员支持列表可在此处找到。


PyTorch本机支持ONNX导出。对于TensorFlow，建议使用tf2onnx。


将模型导出到ONNX后，一个好的第一步是使用多边形图运行恒定折叠。这通常可以解决ONNX解析器中的TensorRT转换问题，并通常简化工作流。有关详细信息，请参阅此示例。在某些情况下，可能需要进一步修改ONNX模型，例如，用插件替换子图，或者根据其他操作重新实现不支持的操作。为了简化此过程，可以使用ONNX GraphSurgeon。

## 1.6. 代码分析工具
有关将valgrind和clang消毒剂工具与TensorRT配合使用的指导，请参阅故障排除一章。

## 1.7. API版本控制
TensorRT版本号（MAJOR.MINOR.PATCH）遵循其公共API和库ABI的语义版本2.0.0。版本号更改如下：

进行不兼容API或ABI更改时的主要版本

以向后兼容的方式添加功能时的次要版本

修复向后兼容的错误时的补丁版本


注意，语义版本控制并没有扩展到序列化对象。要重用计划文件和定时缓存，版本号必须在主版本、次版本、修补程序和生成版本之间匹配。校准缓存通常可以在主版本中重用，但兼容性无法保证。

## 1.8. 弃用政策

弃用用于通知开发人员不再推荐使用某些API和工具。从8.0版开始，TensorRT有以下弃用策略：
弃用通知在发行说明中传达。不推荐使用的API元素在可能的情况下用TRT\U不推荐使用的宏进行标记。
TensorRT在弃用后提供了12个月的迁移期。
API和工具在迁移期间继续工作。
迁移期结束后，将以与语义版本控制一致的方式删除API和工具。
对于TensorRT 7中特别不推荐的任何API和工具。x、 12个月的迁移期从TensorRT 8.0 GA发布日期开始。

## 1.9. 硬件支持寿命

TensorRT 8.5将是支持NVIDIA开普勒（SM 3.x）设备的最新版本。TensorRT 9.0将不再支持Maxwell（SM 5.x）设备。


## 1.10. 支持

有关TensorRT的支持、资源和信息，请访问https://developer.nvidia.com/tensorrt.这包括博客、示例等。


此外，您可以访问NVIDIA DevTalk TensorRT论坛：https://devtalk.nvidia.com/default/board/304/tensorrt/对于所有与TensorRT相关的事情。该论坛提供了寻找答案、建立联系以及参与与客户、开发人员和TensorRT工程师讨论的可能性。

## 1.11. 报告错误

英伟达赞赏所有类型的反馈。如果遇到任何问题，请按照报告TensorRT问题部分中的说明报告问题。